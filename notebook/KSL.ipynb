{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 1: Environment & GPU Setup"
      ],
      "metadata": {
        "id": "B4Iy7hRXwz7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-2aSevtwvsk"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Environment & GPU Setup\n",
        "# ============================================================================\n",
        "# Install/reinstall main packages to ensure compatibility\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install numpy==1.26.4 scipy==1.12.0 scikit-learn==1.5.2\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 2: Configuration & Constants"
      ],
      "metadata": {
        "id": "qPSK0_oLw4Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Configuration & Constants\n",
        "# ============================================================================\n",
        "# Paths\n",
        "BASE_INPUT_DIR = \"/content/drive/MyDrive/KSL_Project/KSL_Videos\"\n",
        "OUTPUT_SKELETON_DIR = \"/content/drive/MyDrive/KSL_Project/KSL_JOINT_STREAM\"\n",
        "\n",
        "# Model parameters\n",
        "SEQUENCE_LENGTH = 32\n",
        "NUM_JOINTS = 47\n",
        "POSE_INDICES = [0, 11, 12, 13, 14]  # Nose, Shoulders, Elbows\n",
        "\n",
        "# Retry settings\n",
        "MAX_RETRIES = 5\n",
        "INITIAL_DELAY_SECONDS = 2\n"
      ],
      "metadata": {
        "id": "jQNEuN1pw3zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 3: MediaPipe Holistic Setup & Landmark Extraction"
      ],
      "metadata": {
        "id": "cKrvCEtzw7xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: MediaPipe Holistic Setup & Landmark Extraction\n",
        "# ============================================================================\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "def extract_landmarks_from_frame(results):\n",
        "    \"\"\"Extracts 47 keypoints from hands + pose.\"\"\"\n",
        "    frame_coords = np.zeros((NUM_JOINTS, 3), dtype=np.float32)\n",
        "\n",
        "    # Left hand\n",
        "    if results.left_hand_landmarks:\n",
        "        for i, lm in enumerate(results.left_hand_landmarks.landmark):\n",
        "            frame_coords[i] = [lm.x, lm.y, lm.z]\n",
        "\n",
        "    # Right hand\n",
        "    if results.right_hand_landmarks:\n",
        "        for i, lm in enumerate(results.right_hand_landmarks.landmark):\n",
        "            frame_coords[i + 21] = [lm.x, lm.y, lm.z]\n",
        "\n",
        "    # Pose keypoints\n",
        "    if results.pose_landmarks:\n",
        "        for i, pose_index in enumerate(POSE_INDICES):\n",
        "            lm = results.pose_landmarks.landmark[pose_index]\n",
        "            frame_coords[i + 42] = [lm.x, lm.y, lm.z]\n",
        "\n",
        "    return frame_coords\n"
      ],
      "metadata": {
        "id": "vnSrRfLnw6OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4: Video Preprocessing Function"
      ],
      "metadata": {
        "id": "Ro8FpP9hw949"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Video Preprocessing Function\n",
        "# ============================================================================\n",
        "def preprocess_video(video_path):\n",
        "    \"\"\"Process video, extract landmarks, and pad/trim to SEQUENCE_LENGTH.\"\"\"\n",
        "    cap = None\n",
        "    delay = INITIAL_DELAY_SECONDS\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if cap.isOpened():\n",
        "            break\n",
        "        print(f\"[Attempt {attempt+1}/{MAX_RETRIES}] Failed to open {video_path}, retrying in {delay}s...\")\n",
        "        cap.release()\n",
        "        time.sleep(delay)\n",
        "        delay *= 2\n",
        "\n",
        "    if not cap or not cap.isOpened():\n",
        "        print(f\"[FATAL] Skipped video: {video_path}\")\n",
        "        return np.zeros((SEQUENCE_LENGTH, NUM_JOINTS, 3), dtype=np.float32)\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    indices_to_sample = np.linspace(0, total_frames - 1, SEQUENCE_LENGTH, dtype=int) if total_frames > 0 else []\n",
        "\n",
        "    joint_data = []\n",
        "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "        current_frame = 0\n",
        "        frame_idx = 0\n",
        "        while cap.isOpened() and frame_idx < SEQUENCE_LENGTH:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if current_frame in indices_to_sample and frame_idx < SEQUENCE_LENGTH:\n",
        "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                image.flags.writeable = False\n",
        "                results = holistic.process(image)\n",
        "                coords = extract_landmarks_from_frame(results)\n",
        "                joint_data.append(coords)\n",
        "                frame_idx += 1\n",
        "            current_frame += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Pad or trim\n",
        "    joint_data = np.array(joint_data, dtype=np.float32)\n",
        "    if joint_data.shape[0] < SEQUENCE_LENGTH:\n",
        "        padding = np.zeros((SEQUENCE_LENGTH - joint_data.shape[0], NUM_JOINTS, 3), dtype=np.float32)\n",
        "        joint_data = np.concatenate([joint_data, padding], axis=0)\n",
        "    elif joint_data.shape[0] > SEQUENCE_LENGTH:\n",
        "        joint_data = joint_data[:SEQUENCE_LENGTH]\n",
        "\n",
        "    return joint_data\n"
      ],
      "metadata": {
        "id": "Iw6gcBFXw-zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 5: Main Video Processing Loop"
      ],
      "metadata": {
        "id": "5cNOLcVLw_6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Main Video Processing Loop\n",
        "# ============================================================================\n",
        "def process_all_videos():\n",
        "    \"\"\"Extract features from all videos recursively and save as pickle.\"\"\"\n",
        "    if not os.path.exists(BASE_INPUT_DIR):\n",
        "        print(f\"Input directory not found: {BASE_INPUT_DIR}\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(OUTPUT_SKELETON_DIR, exist_ok=True)\n",
        "    print(f\"Output directory ready: {OUTPUT_SKELETON_DIR}\")\n",
        "\n",
        "    video_files = glob.glob(os.path.join(BASE_INPUT_DIR, '**', '*.[mM][pP]4'), recursive=True)\n",
        "    if not video_files:\n",
        "        print(f\"No videos found in {BASE_INPUT_DIR}\")\n",
        "        return\n",
        "    print(f\"Found {len(video_files)} video files.\")\n",
        "\n",
        "    all_features, all_labels = [], []\n",
        "    for video_path in tqdm(video_files, desc=\"Processing Videos\"):\n",
        "        try:\n",
        "            class_label = int(os.path.basename(os.path.dirname(video_path)))\n",
        "            joint_data = preprocess_video(video_path)\n",
        "            if np.all(joint_data == 0):\n",
        "                continue\n",
        "            joint_data = joint_data.transpose(2, 0, 1)  # (3, 32, 47)\n",
        "            all_features.append(joint_data)\n",
        "            all_labels.append(class_label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {video_path}: {e}\")\n",
        "\n",
        "    final_features = np.array(all_features, dtype=np.float32)\n",
        "    final_labels = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "    output_pkl_path = os.path.join(OUTPUT_SKELETON_DIR, \"KSL77_joint_stream_47pt.pkl\")\n",
        "    with open(output_pkl_path, 'wb') as f:\n",
        "        pickle.dump((final_features, final_labels), f)\n",
        "\n",
        "    print(f\"\\nProcessed {final_features.shape[0]} videos. Data saved to {output_pkl_path}\")\n",
        "\n",
        "# Run the processing\n",
        "if __name__ == '__main__':\n",
        "    process_all_videos()\n"
      ],
      "metadata": {
        "id": "c3LE8yg9xB9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 6: Load Processed Data & Create Datasets"
      ],
      "metadata": {
        "id": "jjZ9wqPoxDP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Load Processed Data & Create Datasets\n",
        "# ============================================================================\n",
        "data_path = \"/content/drive/MyDrive/KSL_Project/KSL_JOINT_STREAM/KSL77_joint_stream_47pt.pkl\"\n",
        "with open(data_path, \"rb\") as f:\n",
        "    features, labels = pickle.load(f)\n",
        "\n",
        "print(\"Loaded features:\", features.shape)\n",
        "print(\"Loaded labels:\", labels.shape)\n",
        "\n",
        "features = torch.tensor(features, dtype=torch.float32)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "class PoseDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for KSL joint features, with optional augmentation.\"\"\"\n",
        "    def __init__(self, X, y, augment=False):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx].clone()\n",
        "        y = self.y[idx]\n",
        "        if self.augment:\n",
        "            x = self.apply_augmentations(x)\n",
        "        return x, y\n",
        "\n",
        "    def apply_augmentations(self, x):\n",
        "        if random.random() < 0.5: x += torch.randn_like(x) * 0.01\n",
        "        if random.random() < 0.3: x *= 1.0 + (random.random() - 0.5) * 0.1\n",
        "        if random.random() < 0.3: x = torch.roll(x, shifts=random.randint(-2,2), dims=1)\n",
        "        if random.random() < 0.3: x[0] = -x[0]\n",
        "        return x\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, stratify=labels, random_state=42)\n",
        "unique_labels = torch.unique(labels)\n",
        "label_map = {old.item(): new for new, old in enumerate(unique_labels)}\n",
        "y_train = torch.tensor([label_map[int(l)] for l in y_train])\n",
        "y_test = torch.tensor([label_map[int(l)] for l in y_test])\n",
        "reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "num_classes = len(unique_labels)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(PoseDataset(X_train, y_train, augment=True), batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(PoseDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(PoseDataset(X_test, y_test), batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"DataLoaders ready: Train {len(train_loader)}, Val {len(val_loader)}, Test {len(test_loader)}\")\n"
      ],
      "metadata": {
        "id": "VyXhEvitxD4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 7: Model Definition - PoseCNN_LSTM_Attn"
      ],
      "metadata": {
        "id": "D9cV4RNmxFUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Model Definition - PoseCNN_LSTM_Attn\n",
        "# ============================================================================\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PoseCNN_LSTM_Attn(nn.Module):\n",
        "    \"\"\"CNN+LSTM+Attention model for KSL joint sequence classification.\"\"\"\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(1,5), padding=(0,2))\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(1,3), padding=(0,1))\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d((1,2))\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.temp_conv = nn.Conv2d(128,128,kernel_size=(3,1), padding=(1,0))\n",
        "        self.bn_temp = nn.BatchNorm2d(128)\n",
        "        self.lstm = nn.LSTM(input_size=128*(47//2), hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.attn = nn.Sequential(nn.Linear(128*2,128), nn.Tanh(), nn.Linear(128,1))\n",
        "        self.fc = nn.Sequential(nn.BatchNorm1d(128*2), nn.Linear(128*2,256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256,num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn_temp(self.temp_conv(x)))\n",
        "        x = x.permute(0,2,1,3).contiguous()\n",
        "        x = x.view(x.size(0), x.size(1), -1)\n",
        "        out,_ = self.lstm(x)\n",
        "        attn_scores = self.attn(out)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "        context = torch.sum(attn_weights*out, dim=1)\n",
        "        return self.fc(context)\n",
        "\n",
        "model = PoseCNN_LSTM_Attn(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n"
      ],
      "metadata": {
        "id": "Igb5aJwJxGXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 8: Training Loop"
      ],
      "metadata": {
        "id": "O03hLS-0xKUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Training Loop\n",
        "# ============================================================================\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Train model\n",
        "best_acc, patience, trigger = 0, 10, 0\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(f\"Epoch {epoch}: New best accuracy {best_acc:.4f}\")\n",
        "        trigger = 0\n",
        "    else:\n",
        "        trigger += 1\n",
        "        print(f\"No improvement. Early stop counter: {trigger}/{patience}\")\n",
        "        if trigger >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "TrAsk6xlxK72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 9: Test & Evaluation"
      ],
      "metadata": {
        "id": "KqrCBv6qxM0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Test & Evaluation\n",
        "# ============================================================================\n",
        "# Test accuracy\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {100*correct/total:.2f}%\")\n",
        "\n",
        "# Classification report\n",
        "from sklearn.metrics import classification_report\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "report_dict = classification_report(all_labels, all_preds, output_dict=True)\n",
        "perfect_mapped_classes = [int(cls) for cls, metrics in report_dict.items() if cls.isdigit() and metrics['precision']==1.0 and metrics['recall']==1.0]\n",
        "perfect_original_classes = [reverse_label_map[c] for c in perfect_mapped_classes]\n",
        "\n",
        "print(\"Perfectly predicted classes:\", perfect_original_classes)\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Save final model\n",
        "save_path = \"/content/drive/MyDrive/KSL_Project/best_model2.pt\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")\n"
      ],
      "metadata": {
        "id": "w10LhwWqxNNZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}